{
  "model_version": "1.0",
  "debug_mode": false,
  "num_threads": 18,
  "set_cpu_affinity": true,
  "enable_intel_optimization": false,
  "enable_quantization": false,
  "enable_model_quantization": false,
  "enable_input_quantization": false,
  "quantization_dtype": "int8",
  "quantization_config": {
    "quantization_type": "int8",
    "quantization_mode": "dynamic_per_channel",
    "enable_cache": true,
    "cache_size": 819,
    "buffer_size": 1629,
    "use_percentile": true,
    "min_percentile": 0.1,
    "max_percentile": 99.9,
    "error_on_nan": true,
    "error_on_inf": true,
    "outlier_threshold": 3.0,
    "num_bits": 8,
    "optimize_memory": false
  },
  "enable_request_deduplication": false,
  "max_cache_entries": 2000,
  "cache_ttl_seconds": 150,
  "monitoring_window": 50,
  "enable_monitoring": true,
  "monitoring_interval": 5.0,
  "throttle_on_high_cpu": true,
  "cpu_threshold_percent": 90.0,
  "memory_high_watermark_mb": 4096,
  "memory_limit_gb": 50.934643554687504,
  "enable_batching": true,
  "batch_processing_strategy": "adaptive",
  "batch_timeout": 0.05,
  "max_concurrent_requests": 18,
  "initial_batch_size": 64,
  "min_batch_size": 16,
  "max_batch_size": 256,
  "enable_adaptive_batching": true,
  "enable_memory_optimization": true,
  "enable_feature_scaling": true,
  "enable_warmup": true,
  "enable_quantization_aware_inference": false,
  "enable_throttling": true
}
{
  "model_version": "1.0",
  "debug_mode": false,
  "num_threads": 15,
  "set_cpu_affinity": true,
  "enable_intel_optimization": false,
  "enable_quantization": true,
  "enable_model_quantization": true,
  "enable_input_quantization": true,
  "quantization_dtype": "int8",
  "quantization_config": {
    "quantization_type": "int8",
    "quantization_mode": "dynamic_per_channel",
    "enable_cache": true,
    "cache_size": 716,
    "buffer_size": 1426,
    "use_percentile": true,
    "min_percentile": 0.1,
    "max_percentile": 99.9,
    "error_on_nan": true,
    "error_on_inf": true,
    "outlier_threshold": 3.0,
    "num_bits": 8,
    "optimize_memory": false
  },
  "enable_request_deduplication": true,
  "max_cache_entries": 2000,
  "cache_ttl_seconds": 300,
  "monitoring_window": 100,
  "enable_monitoring": true,
  "monitoring_interval": 10.0,
  "throttle_on_high_cpu": true,
  "cpu_threshold_percent": 90.0,
  "memory_high_watermark_mb": 4096,
  "memory_limit_gb": 44.56781311035156,
  "enable_batching": true,
  "batch_processing_strategy": "adaptive",
  "batch_timeout": 0.1,
  "max_concurrent_requests": 15,
  "initial_batch_size": 64,
  "min_batch_size": 16,
  "max_batch_size": 256,
  "enable_adaptive_batching": true,
  "enable_memory_optimization": true,
  "enable_feature_scaling": true,
  "enable_warmup": true,
  "enable_quantization_aware_inference": false,
  "enable_throttling": true
}